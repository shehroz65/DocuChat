{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68413952-58a5-4c33-8c71-3f1e85a9483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4d36d-3540-4221-b83e-ab3bfabf3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to disk\n",
    "# db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# docs = db2.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9ff632-19ad-4912-ae60-693095bc3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMETNS_DB_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962414e6-ee63-4803-b27a-79a46aa162eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocSearchWrapper:\n",
    "    def __init__(self):\n",
    "        # db = Chroma(\n",
    "        #     persist_directory=DOCUMETNS_DB_DIR,\n",
    "        #     # embedding_function=OpenAIEmbeddings(),\n",
    "        #     embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "        # )\n",
    "        # load the document and split it into chunks\n",
    "        loader = PyPDFLoader(\"Q.pdf\")\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # split it into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=20,\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # create the open-source embedding function\n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # load it into Chroma\n",
    "        db = Chroma.from_documents(docs, embedding_function)\n",
    "        \n",
    "\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "        #model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "        use_triton = False\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "        model = AutoGPTQForCausalLM.from_quantized(\n",
    "            model_name_or_path,\n",
    "            use_safetensors=True,\n",
    "            trust_remote_code=True,\n",
    "            device=\"cuda:0\",\n",
    "            use_triton=use_triton,\n",
    "            quantize_config=None,\n",
    "        )\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=4096,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "        )\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "        # Interactive questions and answers\n",
    "        self.CRChain = ConversationalRetrievalChain.from_llm(\n",
    "            # llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"),\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            # condense_question_llm=self.llm\n",
    "            # condense_question_llm=ChatOpenAI(),\n",
    "        )\n",
    "\n",
    "        self.chat_history = []\n",
    "\n",
    "    def getdb(self):\n",
    "        return self.db\n",
    "\n",
    "    def search_docbase(self, query):\n",
    "        result = self.CRChain({\"question\": query, \"chat_history\": self.chat_history})\n",
    "\n",
    "        self.chat_history.append((query, result[\"answer\"]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7a5239-65a7-403b-bf6d-10f9b83efa60",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (871461346.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    if qu            length_function=len,\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc_search = DocSearchWrapper()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        if query == \"clear\":\n",
    "            doc_search.clear_history()\n",
    "            continue\n",
    "        if query.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # Get the answer from the chain\n",
    "        start = time.time()\n",
    "        res = doc_search.search_docbase(query)\n",
    "        print(res)\n",
    "\n",
    "        answer, docs = res[\"answer\"], res[\"source_documents\"]\n",
    "        end = time.time()\n",
    "\n",
    "        # Print the result\n",
    "        print(\"\\n\\n> Question:\")\n",
    "        print(query)\n",
    "        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
    "        print(answer)\n",
    "\n",
    "        # Print the relevant sources used for the answer\n",
    "        print(\"Sources:\\n\")\n",
    "        for document in docs:\n",
    "            # print(\"> \" + document.metadata[\"source\"] + f\": page({document.metadata['page']})\")\n",
    "            print(\"> \" + document.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6ed51-c9c8-4c6d-b6dd-5949e10b4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search = DocSearchWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9be23-8605-4843-9c74-2cfbbb65a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_search.getdb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a414d4-4894-4827-aa00-14fa8185bf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
