{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68413952-58a5-4c33-8c71-3f1e85a9483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4d36d-3540-4221-b83e-ab3bfabf3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to disk\n",
    "# db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# docs = db2.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9ff632-19ad-4912-ae60-693095bc3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOCUMETNS_DB_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962414e6-ee63-4803-b27a-79a46aa162eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocSearchWrapper:\n",
    "    def __init__(self):\n",
    "        # db = Chroma(\n",
    "        #     persist_directory=DOCUMETNS_DB_DIR,\n",
    "        #     # embedding_function=OpenAIEmbeddings(),\n",
    "        #     embedding_function=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "        # )\n",
    "        # load the document and split it into chunks\n",
    "        loader = PyPDFLoader(\"Q.pdf\")\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # split it into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            # Set a really small chunk size, just to show.\n",
    "            chunk_size=100,\n",
    "            chunk_overlap=20,\n",
    "        )\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # create the open-source embedding function\n",
    "        embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # load it into Chroma\n",
    "        db = Chroma.from_documents(docs, embedding_function)\n",
    "        \n",
    "\n",
    "        retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "        #model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "        use_triton = False\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "        model = AutoGPTQForCausalLM.from_quantized(\n",
    "            model_name_or_path,\n",
    "            use_safetensors=True,\n",
    "            trust_remote_code=True,\n",
    "            device=\"cuda:0\",\n",
    "            use_triton=use_triton,\n",
    "            quantize_config=None,\n",
    "        )\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=4096,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15,\n",
    "        )\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "        # Interactive questions and answers\n",
    "        self.CRChain = ConversationalRetrievalChain.from_llm(\n",
    "            # llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"),\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            # condense_question_llm=self.llm\n",
    "            # condense_question_llm=ChatOpenAI(),\n",
    "        )\n",
    "\n",
    "        self.chat_history = []\n",
    "\n",
    "    def getdb(self):\n",
    "        return self.db\n",
    "\n",
    "    def search_docbase(self, query):\n",
    "        result = self.CRChain({\"question\": query, \"chat_history\": self.chat_history})\n",
    "\n",
    "        self.chat_history.append((query, result[\"answer\"]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7a5239-65a7-403b-bf6d-10f9b83efa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at /home/shehroz/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GPTQ/snapshots/ea078917a7e91c896787c73dba935f032ae658e9/model.safetensors were not used when initializing LlamaForCausalLM: {'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.37.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.38.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.36.self_attn.rotary_emb.inv_freq', 'model.layers.32.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.39.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.34.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.33.self_attn.rotary_emb.inv_freq', 'model.layers.35.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq'}. This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n",
      "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  What is your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is your name?', 'chat_history': [('What is your name?', \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\")], 'answer': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\", 'source_documents': [Document(page_content='3', metadata={'source': 'Q.pdf', 'page': 2}), Document(page_content='5', metadata={'source': 'Q.pdf', 'page': 4}), Document(page_content='Inc., 2015.', metadata={'source': 'Q.pdf', 'page': 11})]}\n",
      "\n",
      "\n",
      "> Question:\n",
      "What is your name?\n",
      "\n",
      "> Answer (took 0.33 s.):\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Sources:\n",
      "\n",
      "> Q.pdf\n",
      "> Q.pdf\n",
      "> Q.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  What is Attention Is All You Need?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is Attention Is All You Need?', 'chat_history': [('What is your name?', \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\"), ('What is Attention Is All You Need?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.')], 'answer': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.', 'source_documents': [Document(page_content='self-attention and started', metadata={'source': 'Q.pdf', 'page': 0}), Document(page_content='attention is', metadata={'source': 'Q.pdf', 'page': 3}), Document(page_content='of self-attention we', metadata={'source': 'Q.pdf', 'page': 5})]}\n",
      "\n",
      "\n",
      "> Question:\n",
      "What is Attention Is All You Need?\n",
      "\n",
      "> Answer (took 1.24 s.):\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Sources:\n",
      "\n",
      "> Q.pdf\n",
      "> Q.pdf\n",
      "> Q.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  How many layers is the encoder stacked of?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How many layers is the encoder stacked of?', 'chat_history': [('What is your name?', \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\"), ('What is Attention Is All You Need?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.'), ('How many layers is the encoder stacked of?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.')], 'answer': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.', 'source_documents': [Document(page_content='self-attention and started', metadata={'source': 'Q.pdf', 'page': 0}), Document(page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention', metadata={'source': 'Q.pdf', 'page': 5}), Document(page_content='aligned recurrence and have been shown to perform well on simple-language question answering and', metadata={'source': 'Q.pdf', 'page': 1})]}\n",
      "\n",
      "\n",
      "> Question:\n",
      "How many layers is the encoder stacked of?\n",
      "\n",
      "> Answer (took 0.78 s.):\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Follow Up Input: How many layers is the encoder stacked of?\n",
      "Standalone question: How many layers are there in the encoder stack?\n",
      "Helpful Answer: The encoder has 6 layers.\n",
      "Sources:\n",
      "\n",
      "> Q.pdf\n",
      "> Q.pdf\n",
      "> Q.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  What is the particular attention called?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the particular attention called?', 'chat_history': [('What is your name?', \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\"), ('What is Attention Is All You Need?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.'), ('How many layers is the encoder stacked of?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.'), ('What is the particular attention called?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nFollow Up Input: What is the particular attention called?\\nStandalone question: What is the particular attention mechanism used in the Transformer architecture?\\nHelpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\\nHelpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).')], 'answer': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nFollow Up Input: What is the particular attention called?\\nStandalone question: What is the particular attention mechanism used in the Transformer architecture?\\nHelpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\\nHelpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).', 'source_documents': [Document(page_content='self-attention and started', metadata={'source': 'Q.pdf', 'page': 0}), Document(page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention', metadata={'source': 'Q.pdf', 'page': 5}), Document(page_content='aligned recurrence and have been shown to perform well on simple-language question answering and', metadata={'source': 'Q.pdf', 'page': 1})]}\n",
      "\n",
      "\n",
      "> Question:\n",
      "What is the particular attention called?\n",
      "\n",
      "> Answer (took 2.58 s.):\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Human: How many layers is the encoder stacked of?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Follow Up Input: How many layers is the encoder stacked of?\n",
      "Standalone question: How many layers are there in the encoder stack?\n",
      "Helpful Answer: The encoder has 6 layers.\n",
      "Follow Up Input: What is the particular attention called?\n",
      "Standalone question: What is the particular attention mechanism used in the Transformer architecture?\n",
      "Helpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\n",
      "Helpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).\n",
      "Sources:\n",
      "\n",
      "> Q.pdf\n",
      "> Q.pdf\n",
      "> Q.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  How many parallel attention layers are employed?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/shehroz/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How many parallel attention layers are employed?', 'chat_history': [('What is your name?', \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\"), ('What is Attention Is All You Need?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.'), ('How many layers is the encoder stacked of?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.'), ('What is the particular attention called?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nFollow Up Input: What is the particular attention called?\\nStandalone question: What is the particular attention mechanism used in the Transformer architecture?\\nHelpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\\nHelpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).'), ('How many parallel attention layers are employed?', 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nHuman: What is the particular attention called?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nFollow Up Input: What is the particular attention called?\\nStandalone question: What is the particular attention mechanism used in the Transformer architecture?\\nHelpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\\nHelpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).\\nFollow Up Input: How many parallel attention layers are employed?\\nStandalone question: How many parallel attention layers are used in the Transformer architecture?\\nHelpful Answer: The Transformer architecture employs three parallel attention layers.\\nHelpful Answer: Parallel attention allows the model to attend to multiple parts of the input sequence simultaneously, improving performance and efficiency.')], 'answer': 'Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nHuman: What is the particular attention called?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nHuman: How many layers is the encoder stacked of?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\\n\\naligned recurrence and have been shown to perform well on simple-language question answering and\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nHuman: What is Attention Is All You Need?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\nself-attention and started\\n\\nattention is\\n\\nof self-attention we\\n\\nQuestion: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n\\nHuman: What is your name?\\nAssistant: Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n\\n3\\n\\n5\\n\\nInc., 2015.\\n\\nQuestion: What is your name?\\nHelpful Answer: My name is John Smith.\\nFollow Up Input: What is Attention Is All You Need?\\nStandalone question: What does \"Attention Is All You Need\" refer to?\\n\\nPlease rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\\nHelpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\\nFollow Up Input: How many layers is the encoder stacked of?\\nStandalone question: How many layers are there in the encoder stack?\\nHelpful Answer: The encoder has 6 layers.\\nFollow Up Input: What is the particular attention called?\\nStandalone question: What is the particular attention mechanism used in the Transformer architecture?\\nHelpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\\nHelpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).\\nFollow Up Input: How many parallel attention layers are employed?\\nStandalone question: How many parallel attention layers are used in the Transformer architecture?\\nHelpful Answer: The Transformer architecture employs three parallel attention layers.\\nHelpful Answer: Parallel attention allows the model to attend to multiple parts of the input sequence simultaneously, improving performance and efficiency.', 'source_documents': [Document(page_content='self-attention and started', metadata={'source': 'Q.pdf', 'page': 0}), Document(page_content='layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention', metadata={'source': 'Q.pdf', 'page': 5}), Document(page_content='aligned recurrence and have been shown to perform well on simple-language question answering and', metadata={'source': 'Q.pdf', 'page': 1})]}\n",
      "\n",
      "\n",
      "> Question:\n",
      "How many parallel attention layers are employed?\n",
      "\n",
      "> Answer (took 3.15 s.):\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Human: How many layers is the encoder stacked of?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Follow Up Input: How many layers is the encoder stacked of?\n",
      "Standalone question: How many layers are there in the encoder stack?\n",
      "Helpful Answer: The encoder has 6 layers.\n",
      "Human: What is the particular attention called?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Human: How many layers is the encoder stacked of?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention\n",
      "\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Human: What is Attention Is All You Need?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "self-attention and started\n",
      "\n",
      "attention is\n",
      "\n",
      "of self-attention we\n",
      "\n",
      "Question: Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: What is your name?\n",
      "Assistant: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "Inc., 2015.\n",
      "\n",
      "Question: What is your name?\n",
      "Helpful Answer: My name is John Smith.\n",
      "Follow Up Input: What is Attention Is All You Need?\n",
      "Standalone question: What does \"Attention Is All You Need\" refer to?\n",
      "\n",
      "Please rephrase the follow-up question as a standalone question so it can be answered independently without any additional context.\n",
      "Helpful Answer: \"Attention Is All You Need\" refers to a concept in deep learning called self-attention.\n",
      "Follow Up Input: How many layers is the encoder stacked of?\n",
      "Standalone question: How many layers are there in the encoder stack?\n",
      "Helpful Answer: The encoder has 6 layers.\n",
      "Follow Up Input: What is the particular attention called?\n",
      "Standalone question: What is the particular attention mechanism used in the Transformer architecture?\n",
      "Helpful Answer: The particular attention mechanism used in the Transformer architecture is self-attention.\n",
      "Helpful Answer: Self-attention is a type of attention mechanism used in deep learning models such as transformers, where the model attends to all positions in the input sequence simultaneously, rather than sequentially as in traditional recurrent neural networks (RNNs).\n",
      "Follow Up Input: How many parallel attention layers are employed?\n",
      "Standalone question: How many parallel attention layers are used in the Transformer architecture?\n",
      "Helpful Answer: The Transformer architecture employs three parallel attention layers.\n",
      "Helpful Answer: Parallel attention allows the model to attend to multiple parts of the input sequence simultaneously, improving performance and efficiency.\n",
      "Sources:\n",
      "\n",
      "> Q.pdf\n",
      "> Q.pdf\n",
      "> Q.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m doc_search \u001b[38;5;241m=\u001b[39m DocSearchWrapper()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter a query: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a query:  clear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    doc_search = DocSearchWrapper()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        if query == \"clear\":\n",
    "            doc_search.clear_history()\n",
    "            continue\n",
    "        if query.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        # Get the answer from the chain\n",
    "        start = time.time()\n",
    "        res = doc_search.search_docbase(query)\n",
    "        print(res)\n",
    "\n",
    "        answer, docs = res[\"answer\"], res[\"source_documents\"]\n",
    "        end = time.time()\n",
    "\n",
    "        # Print the result\n",
    "        print(\"\\n\\n> Question:\")\n",
    "        print(query)\n",
    "        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
    "        print(answer)\n",
    "\n",
    "        # Print the relevant sources used for the answer\n",
    "        print(\"Sources:\\n\")\n",
    "        for document in docs:\n",
    "            # print(\"> \" + document.metadata[\"source\"] + f\": page({document.metadata['page']})\")\n",
    "            print(\"> \" + document.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6ed51-c9c8-4c6d-b6dd-5949e10b4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search = DocSearchWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9be23-8605-4843-9c74-2cfbbb65a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_search.getdb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a414d4-4894-4827-aa00-14fa8185bf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
